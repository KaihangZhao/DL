{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Recommendation_System_NeuralNetwork.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPzbNT+qjTYYgfZHQ4P3e8C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KaihangZhao/DL_Notebook_Warehouse/blob/main/Recommendation_System/NeuralNetwork.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zisiPUVBKuQh"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch \n",
        "import torch.autograd as autograd \n",
        "import torch.nn as nn \n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5q-J-95bKv8p"
      },
      "outputs": [],
      "source": [
        "cuda = torch.device('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5f7GoYC8K1M2"
      },
      "outputs": [],
      "source": [
        "item = pd.read_csv('item_feature.csv')\n",
        "train = pd.read_csv('training.csv')\n",
        "df = train.merge(item, on = 'item_id', how = 'left')\n",
        "df['label'] =1\n",
        "train = df.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hg1eeAaMLNu1"
      },
      "outputs": [],
      "source": [
        "u = np.random.randint(low=0.0, high=df.user_id.max(), size=int(len(df)*1))\n",
        "i = np.random.randint(low=0.0, high=df.item_id.max(), size=int(len(df)*1))\n",
        "p = (df.context_feature_id.value_counts()/len(df)).sort_index().values\n",
        "p[p.argmin()]+=1-p.sum()\n",
        "c = np.random.choice(4, len(df)*10, p=p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LrBIlsILT_b"
      },
      "outputs": [],
      "source": [
        "sample= pd.concat([pd.Series(u),pd.Series(i)], axis =1).\\\n",
        "rename(columns={0:'user_id', 1:'item_id'})\n",
        "sample = sample.merge(item, on = 'item_id', how = 'left')\n",
        "sample['label'] = 0\n",
        "df = pd.concat([df,sample])\n",
        "df = df.drop_duplicates(subset=['user_id','item_id']).reset_index(drop = True)\n",
        "df = df[df['label']==0]\n",
        "df = pd.concat([train, df]).reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G5aUDosLLXAM"
      },
      "outputs": [],
      "source": [
        "pos = df[df.label ==1].reset_index(drop = True)\n",
        "neg = df[df.label ==0].reset_index(drop = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Q4n_DLmNxin"
      },
      "outputs": [],
      "source": [
        "def data_sample(pos, neg):\n",
        "\n",
        "  msk = np.random.rand(len(pos)) < 0.8\n",
        "  train_pos = pos[msk].reset_index(drop = True)\n",
        "  val_pos = pos[~msk].reset_index(drop = True)\n",
        "\n",
        "  msk = np.random.rand(len(neg)) < 0.8\n",
        "  train_neg = neg[msk].sample(frac = len(neg)/len(pos)).reset_index(drop = True)\n",
        "  val_neg = neg[~msk].sample(frac = len(neg)/len(pos)).reset_index(drop = True)\n",
        "\n",
        "  train = pd.concat([train_pos, train_neg]).sample(frac=1).reset_index(drop = True)\n",
        "  val = pd.concat([val_pos, val_neg]).sample(frac=1).reset_index(drop = True)\n",
        "  \n",
        "  return train, val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tLmH4KX1MLzE"
      },
      "outputs": [],
      "source": [
        "train, val = data_sample(pos, neg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E0mlmLbPZ5u"
      },
      "outputs": [],
      "source": [
        "class NN(nn.Module):\n",
        "    def __init__(self, num_users, num_items, num_item_feature, emb_user_size=20, emb_item_size=20, emb_item_feature_size=20, seed=23):\n",
        "        super(MF, self).__init__()\n",
        "        torch.manual_seed(seed)\n",
        "        self.user_emb = nn.Embedding(num_users, emb_user_size)\n",
        "        self.item_emb = nn.Embedding(num_items, emb_item_size)\n",
        "        self.item_feature_emb = nn.Embedding(num_item_feature, emb_item_feature_size)\n",
        "        # init \n",
        "        self.item_feature_emb.weight.data.uniform_(0,0.05)\n",
        "        self.user_emb.weight.data.uniform_(0,0.05)\n",
        "        self.item_emb.weight.data.uniform_(0,0.05)\n",
        "        self.classifier = nn.Sigmoid()\n",
        "        self.linear1 = nn.Linear(emb_user_size+emb_item_size+emb_item_feature_size, 2)\n",
        "        self.nonlin = nn.LeakyReLU()\n",
        "        self.linear3 = nn.Linear(2,1)\n",
        "        \n",
        "    def forward(self, u, v, c):\n",
        "        U = self.user_emb(u)\n",
        "        V = self.item_emb(v)\n",
        "        C = self.item_feature_emb(c)\n",
        "        ensemble = torch.cat((U,V,C),dim=1)\n",
        "        pred = self.linear1(ensemble)\n",
        "        pred = self.nonlin(pred)\n",
        "        pred = self.linear3(pred)\n",
        "        self.U = U\n",
        "        self.V = V\n",
        "        self.C = C\n",
        "        return self.classifier(pred.squeeze())\n",
        "        ### END SOLUTION\n",
        "  \n",
        "    def __getitem__(self,idx):\n",
        "\n",
        "        return self.U[idx], self.V[idx], self.C[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HKAZM17zPi5o"
      },
      "outputs": [],
      "source": [
        "def train_one_epoch(model, train_df, optimizer):\n",
        "  tensor_x_tr = torch.LongTensor(np.array(train_df[['user_id', 'item_id', 'item_feature_id']])) # transform to torch tensor\n",
        "  tensor_y_tr = torch.Tensor(train_df['label'])\n",
        "  train_ds = TensorDataset(tensor_x_tr,tensor_y_tr)\n",
        "  train_dl = DataLoader(train_ds, batch_size=50000, shuffle=True)\n",
        "  \"\"\" Trains the model for one epoch\"\"\"\n",
        "  ### BEGIN SOLUTION\n",
        "  losses = []\n",
        "  acc=[]\n",
        "  for x, y in train_dl:\n",
        "      model.train()\n",
        "      y = y.cuda()\n",
        "      u = torch.LongTensor(x[:,0]).cuda()\n",
        "      v = torch.LongTensor(x[:,1]).cuda()\n",
        "      c = torch.LongTensor(x[:,2]).cuda()\n",
        "      y_hat = model(u,v,c)\n",
        "      output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
        "      loss = F.binary_cross_entropy(y_hat, y.float())\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      losses.append(loss.item())\n",
        "      acc.append(accuracy_score(output.cpu().detach().numpy(),y.cpu().detach().numpy()))\n",
        "      \n",
        "  train_loss = np.mean(losses)\n",
        "  train_acc = np.mean(acc)\n",
        "\n",
        "    ### END SOLUTION\n",
        "  return train_loss, train_acc\n",
        "\n",
        "def valid_metrics(model, valid_df):\n",
        "    \"\"\"Computes validation loss and accuracy\"\"\"\n",
        "    model.eval()\n",
        "    ### BEGIN SOLUTION\n",
        "    u = torch.LongTensor(valid_df.user_id.values).cuda()\n",
        "    v = torch.LongTensor(valid_df.item_id.values).cuda()\n",
        "    c = torch.LongTensor(valid_df.item_feature_id.values).cuda()\n",
        "    y = torch.FloatTensor(valid_df.label.values).cuda()\n",
        "    y_hat = model(u,v,c)\n",
        "    valid_loss = F.binary_cross_entropy(y_hat, y)\n",
        "    output = torch.as_tensor(y_hat > 0.5, dtype = torch.int8)\n",
        "    auc = roc_auc_score(y.cpu().detach().numpy(), y_hat.cpu().detach().numpy())\n",
        "    valid_acc = accuracy_score(output.cpu().detach().numpy(),y.cpu().detach().numpy())\n",
        "    ### END SOLUTION\n",
        "    return valid_loss.item(), valid_acc, auc\n",
        "\n",
        "def training(model, pos, neg, epochs=10, lr=0.01, wd=0.0):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    train, val = data_sample(pos, neg)\n",
        "    for i in range(epochs):\n",
        "        train_loss, train_acc = train_one_epoch(model, train, optimizer)\n",
        "        valid_loss, valid_acc, auc = valid_metrics(model, val) \n",
        "        if valid_loss<0.31: \n",
        "          print(\"train loss %.3f train acc %.3f valid loss %.3f valid acc %.3f roc auc acc %.3f\" % (train_loss,train_acc,valid_loss, valid_acc, auc)) \n",
        "          break\n",
        "        if i%2 == 0: \n",
        "          train, val = data_sample(pos, neg)\n",
        "        if i%5 == 0:\n",
        "          print(\"train loss %.3f train acc %.3f valid loss %.3f valid acc %.3f roc auc acc %.3f\" % (train_loss,train_acc,valid_loss, valid_acc, auc)) \n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g17oolHePk_t",
        "outputId": "9cde5c6a-cae3-41b9-b9c9-c66e7a5abd50"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 0.749 train acc 0.500 valid loss 0.746 valid acc 0.500 roc auc acc 0.886\n",
            "train loss 0.647 train acc 0.500 valid loss 0.627 valid acc 0.500 roc auc acc 0.935\n",
            "train loss 0.429 train acc 0.875 valid loss 0.414 valid acc 0.874 roc auc acc 0.945\n",
            "train loss 0.320 train acc 0.879 valid loss 0.314 valid acc 0.880 roc auc acc 0.954\n",
            "train loss 0.305 train acc 0.883 valid loss 0.307 valid acc 0.881 roc auc acc 0.954\n"
          ]
        }
      ],
      "source": [
        "model = MF(df.user_id.max()+1, df.item_id.max()+1, df.item_feature_id.max()+1,\n",
        "           emb_user_size=32, emb_item_size=24, emb_item_feature_size=8).cuda()\n",
        "#optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
        "training(model, pos, neg, epochs=51, lr=0.0005, wd=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uzQvFwfSzenJ",
        "outputId": "74908cb8-3ac7-4c3e-b426-2e8170048061"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss 0.300 train acc 0.884 valid loss 0.300 valid acc 0.883 roc auc acc 0.956\n"
          ]
        }
      ],
      "source": [
        "# stop, but continue training until a second stop\n",
        "training(model, pos, neg, epochs=51, lr=0.0001, wd=1e-6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCqYuDgCuCQ3",
        "outputId": "2207a5a9-73b7-4323-d2eb-e64121dfa574"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5350, 0.2149, 0.8678,  ..., 0.9268, 0.9268, 0.1153], device='cuda:0',\n",
              "       grad_fn=<SigmoidBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "test = pd.read_csv('test_kaggle.csv')\n",
        "test = test.merge(item, on = 'item_id', how = 'left')\n",
        "u = torch.LongTensor(test.user_id.values).cuda()\n",
        "v = torch.LongTensor(test.item_id.values).cuda()\n",
        "c = torch.LongTensor(test.item_feature_id.values).cuda()\n",
        "y_hat = model(u,v,c)\n",
        "y_hat"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.mean(y_hat.cpu().detach().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LbmsnpTq-_8i",
        "outputId": "f6467ac2-36ca-45f5-8f7f-f8cabd79c87f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.51798266"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JX-DhDffH2QR",
        "outputId": "8c1869e5-2a9e-4372-9e5f-9602e76de954"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.5012703698362547"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "prob = pd.Series(y_hat.cpu().detach().numpy()).reset_index().rename(columns = {'index':'id',0:'rating'})\n",
        "sum(prob.rating>0.5)/len(prob)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjOusz6QTBzC"
      },
      "outputs": [],
      "source": [
        "prob.to_csv('trial63.csv',index = False)"
      ]
    }
  ]
}